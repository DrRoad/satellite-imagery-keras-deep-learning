{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "c:\\users\\me\\appdata\\local\\temp\\try_flags_cuvzyt.c:4:19: fatal error: cudnn.h: No such file or directory\n",
      "compilation terminated.\n",
      "\n",
      "Mapped name None to device cuda: GeForce GTX 1060 6GB (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import keras as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import configparser\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from utils.file import makedirs\n",
    "from utils.recorder import record_model_medata, record_model_scores\n",
    "from utils.loader import load_training_set, load_test_set\n",
    "from utils.f2thresholdfinder import *\n",
    "from utils.imagegen import *\n",
    "from utils.models import *\n",
    "from utils.custommetrics import *\n",
    "from utils.samplesduplicator import duplicate_train_samples\n",
    "from utils.training import *\n",
    "from utils.predictor import submission_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading configurations from config file: cfg/default.cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Me\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:10: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training samples: 40479\n",
      "[ True  True  True False False False]\n",
      "model: JAGG_1\n",
      "rescaled dimension: 64\n",
      "batch size: 512\n",
      "rotation_range:20 horizontal_flip:True vertical_flip:True\n"
     ]
    }
   ],
   "source": [
    "config_file = 'cfg/default.cfg'\n",
    "\n",
    "# command line args processing \"python aggregate_model.py cfg/3.cfg\"\n",
    "if len(sys.argv) > 1 and '.cfg' in sys.argv[1]:\n",
    "    config_file = sys.argv[1]\n",
    "\n",
    "print('reading configurations from config file: {}'.format(config_file))\n",
    "\n",
    "settings = configparser.ConfigParser()\n",
    "settings.read(config_file)\n",
    "data_dir = settings.get('data', 'data_dir')\n",
    "\n",
    "df_train = pd.read_csv(data_dir + 'train_v2.csv')\n",
    "model_filename = 'aggregate_model_'+ timestr +'.h5'\n",
    "model_filepath = data_dir + 'models/' + model_filename\n",
    "sample_submission_filepath = data_dir + 'sample_submission_v2.csv'\n",
    "number_of_samples = len(df_train.index)\n",
    "print('total number of samples: {}'.format(number_of_samples))\n",
    "\n",
    "# WARNING: keras allow either 1, 3, or 4 channels per pixel. Other numbers not allowed.\n",
    "data_mask_label = np.array(['R', 'G', 'B', 'NDVI', 'NDWI', 'NIR'])\n",
    "#print(settings.get('data', 'data_mask'))\n",
    "data_mask_list = json.loads(settings.get('data', 'data_mask'))\n",
    "\n",
    "data_mask = ma.make_mask(data_mask_list)\n",
    "print(data_mask)\n",
    "\n",
    "num_channels = np.sum(data_mask)\n",
    "need_norm_stats = False\n",
    "\n",
    "model_id = settings.get('model', 'model_id')\n",
    "print('model: {}'.format(model_id))\n",
    "\n",
    "# default to 64\n",
    "rescaled_dim = 64\n",
    "if settings.has_option('data', 'rescaled_dim'):\n",
    "    rescaled_dim = settings.getint('data', 'rescaled_dim')\n",
    "print('rescaled dimension: {}'.format(rescaled_dim))\n",
    "\n",
    "# one epoch is an arbitrary cutoff : one pass over the entire training set\n",
    "number_epoch = settings.getint('model', 'number_epoch')\n",
    "\n",
    "# a batch results in exactly one update to the model.\n",
    "# batch_size is limited by model size and GPU memory\n",
    "batch_size = settings.getint('model', 'batch_size') \n",
    "print('batch size: {}'.format(batch_size))\n",
    "\n",
    "classifier_threshold = 0.2 # used for end of epoch f2 approximation only\n",
    "\n",
    "split = int(number_of_samples * 0.80)  # TODO we may want to increase to 0.90 eventually\n",
    "number_validations = number_of_samples - split\n",
    "\n",
    "has_augmentation_config = settings.has_section('augmentation')\n",
    "if has_augmentation_config:\n",
    "    rotation_range = settings.getint('augmentation', 'rotation_range')\n",
    "    horizontal_flip = settings.getboolean('augmentation', 'horizontal_flip')\n",
    "    vertical_flip = settings.getboolean('augmentation', 'vertical_flip')\n",
    "    print('rotation_range:{} horizontal_flip:{} vertical_flip:{}'.format(rotation_range,horizontal_flip,vertical_flip))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slash_burn', 'clear', 'blooming', 'primary', 'cloudy', 'conventional_mine', 'water', 'haze', 'cultivation', 'partly_cloudy', 'artisinal_mine', 'habitation', 'bare_ground', 'blow_down', 'agriculture', 'road', 'selective_logging']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "labels = list(set(flatten([l.split(' ') for l in df_train['tags'].values])))\n",
    "\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label_map = {l: i for i, l in enumerate(labels)}\n",
    "#inv_label_map = {i: l for l, i in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479L, 64L, 64L, 6L)\n",
      "(40479L, 17L)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_training_set(df_train, rescaled_dim)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479L, 3L, 64L, 64L)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train[:, :, :, data_mask]\n",
    "\n",
    "x_train = x_train.transpose(0,3,1,2)  # https://github.com/fchollet/keras/issues/2681\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO save the shuffling order to hdf5 so we can recreate the training and validation sets post execution.\n",
    "\n",
    "# shuffle the samples because \n",
    "# 1) the original samples may not be randomized & \n",
    "# 2) to avoid the possiblility of overfitting the validation data while we tune the model\n",
    "from sklearn.utils import shuffle\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = x_train[:split], x_train[split:], y_train[:split], y_train[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32383L, 3L, 64L, 64L)\n",
      "(32383L, 17L)\n",
      "(8096L, 3L, 64L, 64L)\n",
      "(8096L, 17L)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41167L, 3L, 64L, 64L)\n",
      "(41167L, 17L)\n"
     ]
    }
   ],
   "source": [
    "# experimental hack to get more samples for augmentations for a specific low-frequency tag in unbalanced dataset. e.g. habitation\n",
    "# selecting the optimal multiplier is sensitive\n",
    "\n",
    "augmentation_hack_config = settings.has_section('augmentation_hack')\n",
    "if augmentation_hack_config:\n",
    "    dup_multiplier = settings.getint('augmentation_hack', 'multiplier')\n",
    "    hack_label_target = settings.get('augmentation_hack', 'label_target')\n",
    "\n",
    "if augmentation_hack_config:\n",
    "    x_train, y_train = duplicate_train_samples(x_train, y_train, labels.index(hack_label_target), multiplier=dup_multiplier)\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dynamicly set num_samples_per_epoch\n",
    "# TODO understand the implications of num_samples_per_epoch.\n",
    "# +0.002 to +0.01??? F2 score improvement when number_of_samples * 3\n",
    "# num_samples_per_epoch = 1000\n",
    "num_samples_per_epoch = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('image generator', rotation_range:20 horizontal_flip:True vertical_flip:True)\n"
     ]
    }
   ],
   "source": [
    "def get_img_generator():\n",
    "    if has_augmentation_config:\n",
    "        return GeneralImgGen(rotation_range = rotation_range, \n",
    "                             horizontal_flip = horizontal_flip, \n",
    "                             vertical_flip = vertical_flip)\n",
    "    else:\n",
    "        return ScaledDown() # default\n",
    "    \n",
    "image_generator = get_img_generator()\n",
    "print('image generator', image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = image_generator.getTrainGenenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (need_norm_stats):\n",
    "    # need to compute internal stats like featurewise std and zca whitening\n",
    "    train_datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_datagen = image_generator.getValidationGenenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# workaround to provide your own stats: \n",
    "# http://stackoverflow.com/questions/41855512/how-does-data-normalization-work-in-keras-during-prediction/43069409#43069409\n",
    "if (need_norm_stats):\n",
    "    # need to compute internal stats like featurewise std and zca whitening\n",
    "    validation_datagen.fit(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_generator = validation_datagen.flow(\n",
    "        x_valid,\n",
    "        y_valid,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_model(model_id, num_channels, rescaled_dim, rescaled_dim)\n",
    "\n",
    "# https://github.com/fchollet/keras/issues/369\n",
    "# https://github.com/fchollet/keras/blob/master/keras/losses.py\n",
    "model.compile(loss='binary_crossentropy', # Is this the best loss function?\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'recall', 'precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BUG when resuming training, the learning rate need to be decreased.\n",
    "# let's load an existing trained model and continue training more epoch gives 0.01 improvement in LB score.\n",
    "# model = load_model(data_dir + 'models/aggregate_model_20170507-124128.h5') # 0.86\n",
    "# model = load_model(data_dir + 'models/aggregate_model_20170507-184232.h5') # 0.87\n",
    "# model = load_model(data_dir + 'models/aggregate_model_20170511-133235.h5')\n",
    "# model = load_model(data_dir + 'models/aggregate_model_20170515-062741.h5')\n",
    "#number_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ran into MemoryError when training DAGG_2 with 4 channels at epoch 50.\n",
    "# To try to get reduce memory usage, limit the number of samples and batch_size\n",
    "\n",
    "validation_num_samples = min(1280, number_of_samples - split)\n",
    "x_valid_f2 = x_valid[:validation_num_samples]\n",
    "y_valid_f2 = y_valid[:validation_num_samples]\n",
    "\n",
    "# Note: threshold is fixed (not optimized per label)\n",
    "def compute_f2_measure(l_model):    \n",
    "    val_generator_f2 = validation_datagen.flow(\n",
    "        x_valid_f2,\n",
    "        y_valid_f2,\n",
    "        batch_size=64,\n",
    "        shuffle=False)\n",
    "    raw_pred = l_model.predict_generator(val_generator_f2, validation_num_samples)\n",
    "    thresholded_pred = (np.array(raw_pred) > classifier_threshold).astype(int)\n",
    "    l_f2_score = fbeta_score(y_valid_f2, thresholded_pred, beta=2, average='samples')\n",
    "    return l_f2_score\n",
    "    \n",
    "class F2_Validation(k.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f2_measures = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.f2_measures.append(compute_f2_measure(self.model))\n",
    "\n",
    "f2_score_val = F2_Validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stop overfitting on training data\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=3, min_delta=0, verbose=0, mode='auto')  # TODO patience and min_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 512/1000 [==============>...............] - ETA: 1s - loss: 0.6954 - acc: 0.4999 - recall: 0.4251 - precision: 0.1780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Me\\Anaconda2\\lib\\site-packages\\keras\\keras\\engine\\training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1000 [==============================]\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 14s - loss: 0.6884 - acc: 0.5749 - recall: 0.3801 - precision: 0.1999 - val_loss: 0.6639 - val_acc: 0.8157 - val_recall: 0.3101 - val_precision: 0.4353\n",
      "Epoch 2/2\n",
      "1024/1000 [==============================] - ETA: 0s - loss: 0.6651 - acc: 0.6803 - recall: 0.2755 - precision: 0.2276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b - 11s - loss: 0.6532 - acc: 0.6884 - recall: 0.2819 - precision: 0.2467 - val_loss: 0.6032 - val_acc: 0.8182 - val_recall: 0.3100 - val_precision: 0.4443\n",
      "model training complete\n"
     ]
    }
   ],
   "source": [
    "training_start_time = datetime.now()\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(train_generator,\n",
    "                    samples_per_epoch=num_samples_per_epoch,\n",
    "                    nb_epoch=number_epoch,\n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=number_validations,\n",
    "                    callbacks=[f2_score_val, early_stop])\n",
    "\n",
    "model.save(model_filepath)  # always save your model and weights after training or during training\n",
    "time_spent_trianing = datetime.now() - training_start_time\n",
    "\n",
    "print('model training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 62, 62)    896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 62, 62)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 32, 31, 31)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 32, 29, 29)    9248        maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 29, 29)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 32, 14, 14)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, 12, 12)    18496       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 64, 12, 12)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 64, 6, 6)      0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 4, 4)     73856       maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 128, 4, 4)     0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 128, 2, 2)     0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            32832       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 17)            1105        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 136,433\n",
      "Trainable params: 136,433\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# model = load_model(data_dir + 'models/aggregate_model_20170517-062305.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:0 threshold:0.49 score:0.331566096909\n",
      "label:1 threshold:0.0 score:0.331566096909\n",
      "label:2 threshold:0.51 score:0.33156970854\n",
      "label:3 threshold:0.0 score:0.641839226104\n",
      "label:4 threshold:0.46 score:0.641906131572\n",
      "label:5 threshold:0.44 score:0.641906131572\n",
      "label:6 threshold:0.52 score:0.657018505906\n",
      "label:7 threshold:0.49 score:0.657018505906\n",
      "label:8 threshold:0.5 score:0.657018505906\n",
      "label:9 threshold:0.0 score:0.669730908111\n",
      "label:10 threshold:0.48 score:0.669730908111\n",
      "label:11 threshold:0.5 score:0.669730908111\n",
      "label:12 threshold:0.48 score:0.669730908111\n",
      "label:13 threshold:0.47 score:0.669730908111\n",
      "label:14 threshold:0.36 score:0.695330433644\n",
      "label:15 threshold:0.49 score:0.69542487635\n",
      "label:16 threshold:0.51 score:0.695427964294\n",
      "('>>>> Overall F2 score over validation set using samples averaging ', 0.69542796429448317)\n"
     ]
    }
   ],
   "source": [
    "# use the validation data to compute some stats which tell us how the model is performing on the validation data set.\n",
    "val_generator_score_board = validation_datagen.flow(\n",
    "    x_valid,\n",
    "    y_valid,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "p_valid = model.predict_generator(val_generator_score_board, number_validations)\n",
    "\n",
    "optimized_thresholds = f2_optimized_thresholds(y_valid, np.array(p_valid))\n",
    "\n",
    "y_predictions = (np.array(p_valid) > optimized_thresholds).astype(int)\n",
    "\n",
    "# F2 score, which gives twice the weight to recall emphasising recall higher than precision\n",
    "# 'samples' is what the evaluation criteria is for the contest\n",
    "f2_score = fbeta_score(y_valid, y_predictions, beta=2, average='samples')\n",
    "print('>>>> Overall F2 score over validation set using samples averaging ' , f2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                label  optimized_threshold\n",
      "0          slash_burn                 0.49\n",
      "1               clear                 0.00\n",
      "2            blooming                 0.51\n",
      "3             primary                 0.00\n",
      "4              cloudy                 0.46\n",
      "5   conventional_mine                 0.44\n",
      "6               water                 0.52\n",
      "7                haze                 0.49\n",
      "8         cultivation                 0.50\n",
      "9       partly_cloudy                 0.00\n",
      "10     artisinal_mine                 0.48\n",
      "11         habitation                 0.50\n",
      "12        bare_ground                 0.48\n",
      "13          blow_down                 0.47\n",
      "14        agriculture                 0.36\n",
      "15               road                 0.49\n",
      "16  selective_logging                 0.51\n"
     ]
    }
   ],
   "source": [
    "threshold_df = pd.DataFrame({'label':labels, \n",
    "                             'optimized_threshold':optimized_thresholds})\n",
    "print(threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Me\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Me\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                label    f2  recall  precision  true_sum  predict_sum\n",
      "0          slash_burn  0.00    0.00       0.00        44            0\n",
      "1               clear  0.92    1.00       0.70      5684         8096\n",
      "2            blooming  0.00    0.00       0.00        66            0\n",
      "3             primary  0.98    1.00       0.92      7451         8096\n",
      "4              cloudy  0.00    0.00       0.33       452            3\n",
      "5   conventional_mine  0.00    0.00       0.00        22            0\n",
      "6               water  0.55    0.83       0.23      1513         5437\n",
      "7                haze  0.00    0.00       0.00       527            0\n",
      "8         cultivation  0.00    0.00       0.00       874            0\n",
      "9       partly_cloudy  0.52    1.00       0.18      1433         8096\n",
      "10     artisinal_mine  0.00    0.00       0.00        70            0\n",
      "11         habitation  0.00    0.00       0.00       732            0\n",
      "12        bare_ground  0.00    0.00       0.00       170            0\n",
      "13          blow_down  0.00    0.00       0.00        23            0\n",
      "14        agriculture  0.68    1.00       0.30      2449         8085\n",
      "15               road  0.01    0.01       0.25      1635           57\n",
      "16  selective_logging  0.00    0.00       0.00        74            0\n"
     ]
    }
   ],
   "source": [
    "precision_l, recall_l, f2_score_l = calculate_stats_for_prediction(y_valid, y_predictions)\n",
    "\n",
    "prediction_stats_df = pd.DataFrame({\n",
    "    'label': labels, \n",
    "    'true_sum': np.sum(y_valid, axis=0),\n",
    "    'predict_sum': np.sum(y_predictions, axis=0),\n",
    "    'f2': f2_score_l,\n",
    "    'recall': recall_l,\n",
    "    'precision': precision_l\n",
    "})\n",
    "\n",
    "# reordering the columns for easier reading\n",
    "prediction_stats_df = prediction_stats_df[['label', 'f2', 'recall', 'precision', 'true_sum', 'predict_sum']]\n",
    "print(prediction_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_data_mask_label = data_mask_label[data_mask]\n",
    "\n",
    "record_model_scores(model_filename, \n",
    "                    model_id, \n",
    "                    history, \n",
    "                    f2_score, \n",
    "                    time_spent_trianing, \n",
    "                    num_channels,\n",
    "                    config_file,\n",
    "                    np.array_str(filtered_data_mask_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training history stats:\n",
      "['acc', 'loss', 'recall', 'precision', 'val_acc', 'val_recall', 'val_precision', 'val_loss']\n"
     ]
    }
   ],
   "source": [
    "figures_dir = 'figures/' + model_id\n",
    "makedirs(figures_dir)\n",
    "\n",
    "# list all data in history\n",
    "print('training history stats:')\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for f2 score\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "subplot0 = fig.add_subplot(231)\n",
    "if hasattr(f2_score_val, 'f2_measures'):\n",
    "    subplot0.plot(f2_score_val.f2_measures)\n",
    "subplot0.set_title('f2 score')\n",
    "subplot0.set_ylabel('f2 score')\n",
    "subplot0.set_xlabel('epoch')\n",
    "subplot0.legend(['val'], loc='upper left')\n",
    "\n",
    "# summarize history for recall\n",
    "subplot3 = fig.add_subplot(232)\n",
    "subplot3.plot(history.history['recall'])\n",
    "subplot3.plot(history.history['val_recall'])\n",
    "subplot3.set_title('recall')\n",
    "subplot3.set_ylabel('recall')\n",
    "subplot3.set_xlabel('epoch')\n",
    "subplot3.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# summarize history for precision\n",
    "subplot2 = fig.add_subplot(233)\n",
    "subplot2.plot(history.history['precision'])\n",
    "subplot2.plot(history.history['val_precision'])\n",
    "subplot2.set_title('precision')\n",
    "subplot2.set_ylabel('precision')\n",
    "subplot2.set_xlabel('epoch')\n",
    "subplot2.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# summarize history for accuracy\n",
    "subplot1 = fig.add_subplot(234)\n",
    "subplot1.plot(history.history['acc'])\n",
    "subplot1.plot(history.history['val_acc'])\n",
    "subplot1.set_title('accuracy')\n",
    "subplot1.set_ylabel('accuracy')\n",
    "subplot1.set_xlabel('epoch')\n",
    "subplot1.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "subplot4 = fig.add_subplot(235)\n",
    "subplot4.plot(history.history['loss'])\n",
    "subplot4.plot(history.history['val_loss'])\n",
    "subplot4.set_title('model loss')\n",
    "subplot4.set_ylabel('loss')\n",
    "subplot4.set_xlabel('epoch')\n",
    "subplot4.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "# precision and recall for each label\n",
    "subplot5 = fig.add_subplot(236)\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(prediction_stats_df['label'])))\n",
    "subplot5.scatter(prediction_stats_df['precision'], prediction_stats_df['recall'], c=colors)\n",
    "subplot5.set_title('precision & recall')\n",
    "subplot5.set_xlabel('precision')\n",
    "subplot5.set_ylabel('recall')\n",
    "for i, txt in enumerate(prediction_stats_df['label']):\n",
    "    subplot5.annotate(txt, (prediction_stats_df['precision'][i], prediction_stats_df['recall'][i]))\n",
    "\n",
    "fig.savefig(figures_dir + '/stats_' + timestr + '.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 126, 126)  2368        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 64, 126, 126)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 63, 63)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 61, 61)    36928       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 64, 61, 61)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 30, 30)    0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 28, 28)   73856       maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 128, 28, 28)   0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 128, 14, 14)   0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 256, 12, 12)   295168      maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 256, 12, 12)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 256, 6, 6)     0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 9216)          0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           1179776     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 17)            2193        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,590,289\n",
      "Trainable params: 1,590,289\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "#model = load_model(data_dir + 'models/aggregate_model_20170521-141533.h5')\n",
    "#print(model.summary())\n",
    "\n",
    "# copied manually from stout\n",
    "#recorded_thresholds = [0.13, 0.13, 0.1, 0.14, 0.05, 0.23, 0.17, 0.15, 0.18, 0.23, 0.09, 0.21, 0.17, 0.10, 0.20, 0.23, 0.1]\n",
    "\n",
    "#image_generator = get_img_generator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the configuration we will use for testing:\n",
    "testset_datagen = image_generator.getTestGenenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of test samples:61191\n",
      "full batches:61 reminder:191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 62/62 [06:37<00:00,  4.97s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 61191/61191 [00:56<00:00, 1074.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# populate the test dataset cache\n",
    "# df_test = pd.read_csv(sample_submission_filepath)\n",
    "# load_test_set(df_test, rescaled_dim)\n",
    "\n",
    "submit_df = submission_dataframe(model,\n",
    "                                 optimized_thresholds,\n",
    "                                 data_mask,\n",
    "                                 testset_datagen, \n",
    "                                 rescaled_dim, \n",
    "                                 labels, \n",
    "                                 sample_submission_filepath,\n",
    "                                 need_norm_stats)\n",
    "submit_df.to_csv(data_dir + 'my_submissions/submission_' + timestr + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test code\n",
    "# nums_ones = np.ones((1, 17))\n",
    "# nums_zeros = np.zeros((1, 17))\n",
    "# haha = np.array([[1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]])\n",
    "\n",
    "# y_testset_predictions = haha\n",
    "# result = pd.DataFrame(y_testset_predictions, columns = labels)\n",
    "\n",
    "# preds = []\n",
    "# for i in tqdm(range(result.shape[0]), miniters=1000):\n",
    "#     a = result.ix[[i]]\n",
    "#     #print(a)\n",
    "#     a = a.transpose()\n",
    "#     print(a)\n",
    "#     a = a.loc[a[i] == 1]\n",
    "#     print(a)\n",
    "#     ' '.join(list(a.index))\n",
    "#     preds.append(' '.join(list(a.index)))\n",
    "    \n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent to complete execution: 0:05:27.612000\n"
     ]
    }
   ],
   "source": [
    "total_exec_time = datetime.now() - start_time\n",
    "print ('time spent to complete execution: {}'.format(total_exec_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
