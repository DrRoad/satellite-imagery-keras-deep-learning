reading configurations from config file: cfg/tune_vgg16_aggressive5.cfg
model: vgg16
source model file: D:/Downloads/amazon/bottleneck/vgg16/frozen19_20170704-132538.h5
number of frozen layers: 15
[0.0001, 3e-05]
[100, 10]
batch size: 64
(40479L, 224L, 224L, 3L)
(40479L, 17L)
(0, u'input_1', False)
(1, u'block1_conv1', False)
(2, u'block1_conv2', False)
(3, u'block1_pool', False)
(4, u'block2_conv1', False)
(5, u'block2_conv2', False)
(6, u'block2_pool', False)
(7, u'block3_conv1', False)
(8, u'block3_conv2', False)
(9, u'block3_conv3', False)
(10, u'block3_pool', False)
(11, u'block4_conv1', False)
(12, u'block4_conv2', False)
(13, u'block4_conv3', False)
(14, u'block4_pool', False)
(15, u'block5_conv1', True)
(16, u'block5_conv2', True)
(17, u'block5_conv3', True)
(18, u'block5_pool', True)
(19, u'flatten_1', True)
(20, u'dense_1', True)
(21, u'dropout_1', True)
(22, u'dense_2', True)
(23, u'dropout_2', True)
(24, u'dense_3', True)
learning rate :0.0001
Epoch 00000: val_loss improved from inf to 0.10233, saving model to D:/Downloads/amazon/bottleneck/vgg16/frozen15_20170706-140457.h5
Epoch 00001: val_loss improved from 0.10233 to 0.09756, saving model to D:/Downloads/amazon/bottleneck/vgg16/frozen15_20170706-140457.h5
Epoch 00002: val_loss improved from 0.09756 to 0.09588, saving model to D:/Downloads/amazon/bottleneck/vgg16/frozen15_20170706-140457.h5
Epoch 00003: val_loss did not improve
Epoch 00004: val_loss did not improve
learning rate :3e-05
Epoch 00000: val_loss improved from 0.09588 to 0.09225, saving model to D:/Downloads/amazon/bottleneck/vgg16/frozen15_20170706-140457.h5
Epoch 00001: val_loss did not improve
Epoch 00002: val_loss did not improve
vgg16 model training complete. Time taken: 1:12:04.195000
loading model: D:/Downloads/amazon/bottleneck/vgg16/frozen15_20170706-140457.h5
