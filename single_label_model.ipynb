{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "c:\\users\\me\\appdata\\local\\temp\\try_flags_rpdq_c.c:4:19: fatal error: cudnn.h: No such file or directory\n",
      "compilation terminated.\n",
      "\n",
      "Mapped name None to device cuda: GeForce GTX 1060 6GB (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "# Training a small convnet from scratch to classify a single binary label (e.g. agriculture)\n",
    "# See: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "\n",
    "pal = sns.color_palette()\n",
    "\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Me\\kaggle\\amazon\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.recorder import test_import\n",
    "test_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'D:/Downloads/amazon/'\n",
    "target_label = 'agriculture'\n",
    "num_samples_per_epoch = 60000\n",
    "# when > 10000 , notebook freezes, use command line\n",
    "num_epoch = 20\n",
    "train_dir = data_dir + 'prep/' + target_label + '/train-jpg'\n",
    "validation_dir = data_dir + 'prep/' + target_label + '/validation-jpg'\n",
    "batch_size = 64  # limited by GPU memory size and model size\n",
    "num_validations_per_epoch = 600\n",
    "# all images will be resized to 128x128, which reduces number of pixels by 4x.\n",
    "rescaled_dim = 128\n",
    "model_file = data_dir + 'models/' + target_label + '-' + timestr + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(3, rescaled_dim, rescaled_dim)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',  # was binary_crossentropy\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'recall', 'precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32383 images belonging to 2 classes.\n",
      "Found 8096 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "# TODO add rotation for small data set\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "# this is the configuration we will use for testing:\n",
    "# only rescaling\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolders of train_dir, and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(rescaled_dim, rescaled_dim),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(rescaled_dim, rescaled_dim),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 64/100 [==================>...........] - ETA: 1s - loss: 0.6861 - acc: 0.6094 - recall: 0.1364 - precision: 0.3333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Me\\Anaconda2\\lib\\site-packages\\keras\\keras\\engine\\training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/100 [======================================] - 15s - loss: 0.6971 - acc: 0.6484 - recall: 0.0682 - precision: 0.1667 - val_loss: 0.6502 - val_acc: 0.6766 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 2/2\n",
      "128/100 [======================================] - 12s - loss: 0.6456 - acc: 0.7266 - recall: 0.0000e+00 - precision: 0.0000e+00 - val_loss: 0.6232 - val_acc: 0.6844 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=num_samples_per_epoch, #  not using all training samples at every epoch.\n",
    "        nb_epoch=num_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=num_validations_per_epoch)\n",
    "model.save(model_file)  # always save your model and weights after training or during training\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          model_file  accuracy  precision  \\\n",
      "0  D:/Downloads/amazon/models/agriculture-2017050...  0.726562        0.0   \n",
      "\n",
      "   recall  \n",
      "0     0.0  \n"
     ]
    }
   ],
   "source": [
    "from utils.recorder import record_model_medata\n",
    "\n",
    "time_spent_trianing = datetime.now() - startTime\n",
    "\n",
    "record_model_medata(model_file, history, time_spent_trianing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for precision\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.title('model precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for recall\n",
    "plt.plot(history.history['recall'])\n",
    "plt.plot(history.history['val_recall'])\n",
    "plt.title('model recall')\n",
    "plt.ylabel('recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model(model_file)\n",
    "#model = load_model(data_dir + 'models/agriculture-20170505-234342.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_10 (Convolution2D) (None, 32, 126, 126)  896         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 32, 126, 126)  0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 32, 63, 63)    0           activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 32, 61, 61)    9248        maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 32, 61, 61)    0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 32, 30, 30)    0           activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 64, 28, 28)    18496       maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 64, 28, 28)    0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_12 (MaxPooling2D)   (None, 64, 14, 14)    0           activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 12544)         0           maxpooling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 64)            802880      flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 64)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 64)            0           activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             65          dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 1)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 831,585\n",
      "Trainable params: 831,585\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40669 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testset_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "testset_dir = data_dir + 'test'\n",
    "\n",
    "testset_generator = testset_datagen.flow_from_directory(\n",
    "        testset_dir,\n",
    "        target_size=(rescaled_dim, rescaled_dim),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset_predict = model.predict_generator(testset_generator, 40669)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_threshold = 0.5 # TODO https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/discussion/32475\n",
    "y_testset_predictions = (np.array(testset_predict) > classifier_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('total time spent to complete execution:' , datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
